{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"scraped_data\"  \n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "os.chdir(f'{curr}/scraped_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yings/Desktop/govtech/section2/scraped_data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search parameters submitted.\n",
      "There are 10 pages to click through.\n"
     ]
    }
   ],
   "source": [
    "## given to scrape for parliament speeches and staements ##\n",
    " \n",
    "driver = webdriver.Chrome()  # Modify 'path_to_chromedriver' accordingly\n",
    "page_url = 'https://sprs.parl.gov.sg/search/home'\n",
    "driver.get(page_url)\n",
    "\n",
    "# Get search box and fill it up\n",
    "search_selector = '''#divmpscreen2 > div.row > div:nth-child(1) > div > div:nth-child(1) > input'''\n",
    "search = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, search_selector))\n",
    ")\n",
    "search.send_keys('COS')\n",
    "\n",
    "# Uncomment following lines to only search in titles\n",
    "# checkbox_selector = '''#divmpscreen2 > div.row > div:nth-child(1) > div > div:nth-child(2) > label > input'''\n",
    "# checkbox = driver.find_element(By.CSS_SELECTOR, checkbox_selector)\n",
    "# checkbox.click()\n",
    "\n",
    "# Select the 13th parliament\n",
    "session_selector = '''#divmpscreen2 > div.row > div:nth-child(1) > div > div.form-group.byParText > select > option:nth-child(14)'''\n",
    "session = driver.find_element(By.CSS_SELECTOR, session_selector)\n",
    "session.click()\n",
    "\n",
    "# Find submit element and click\n",
    "time.sleep(2)  \n",
    "\n",
    "# Now find and click the submit button\n",
    "submit_selector = '''#divmpscreen2 > div.row > div.col-sm-12.text-right.pull-right > div > button:nth-child(2)'''\n",
    "submit = driver.find_element(By.CSS_SELECTOR, submit_selector)\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", submit)\n",
    "time.sleep(1)  \n",
    "submit.click()\n",
    "\n",
    "\n",
    "print('Search parameters submitted.')\n",
    "\n",
    "# Create empty dictionary to store results\n",
    "res_dict = {}\n",
    "\n",
    "# Switch window and check for number of search results\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "num_results_selector = '''#searchResults > div:nth-child(1) > div'''\n",
    "\n",
    "num_results = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, num_results_selector)))\n",
    "time.sleep(1)\n",
    "# Re-fetch the element\n",
    "num_results = driver.find_element(By.CSS_SELECTOR, num_results_selector)\n",
    "res = num_results.text.split(' ')\n",
    "num_clicks = int(res[-1]) // int(res[-3]) + 1\n",
    "print(f'There are {num_clicks} pages to click through.')\n",
    "\n",
    "# Nested for loop to click through all search results\n",
    "for click in range(num_clicks):\n",
    "\n",
    "    for item in range(1, 21):\n",
    "        try:\n",
    "            elem = WebDriverWait(driver, 30).until(\n",
    "                EC.presence_of_element_located((By.XPATH, f'//*[@id=\"searchResults\"]/table/tbody[{item}]/tr[1]/td[2]/a'))\n",
    "            )\n",
    "            \n",
    "            # Scroll the element into view to ensure it's clickable\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", elem)\n",
    "            time.sleep(2) \n",
    "\n",
    "            elem.click()\n",
    "\n",
    "            # Switch to new window/tab\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(2)  \n",
    "\n",
    "            item_key = driver.current_url.split('/')[-1]\n",
    "            item_key = item_key.replace('?', '_')\n",
    "\n",
    "            res_dict[item_key] = driver.page_source\n",
    "\n",
    "            with open(f'{item_key}.txt', encoding='utf-8', mode='w+') as file:\n",
    "                file.write(driver.page_source)\n",
    "\n",
    "            driver.close()\n",
    "\n",
    "            # Switch back to the original window/tab\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "\n",
    "        except:\n",
    "            continue  \n",
    "\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3) \n",
    "\n",
    "    # Click on next page once 20 results have been saved\n",
    "    try:\n",
    "        next_page = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"searchResults\"]/div[3]/section/ul/li[3]/a/em'))\n",
    "        )\n",
    "    except:\n",
    "        next_page = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"searchResults\"]/div[3]/section/ul/li[1]/a/em'))\n",
    "        )\n",
    "\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_page)\n",
    "    time.sleep(2)\n",
    "    driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(res_dict.keys()) == int(res[-1]), \"It looks like not all the results were stored!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(html_string):\n",
    "    soup = bs(html_string, 'html.parser')\n",
    "    sitting_date = soup.find('td', string='Sitting Date:').find_next('td').span.get_text(strip=True)\n",
    "    section_name = soup.find('td', string='Section Name:').find_next('td').span.get_text(strip=True)\n",
    "    mps_speaking = soup.find('td', string='MPs Speaking:').find_next('td').span.get_text(strip=True)\n",
    "    text_content = soup.select_one('div.hansardContent div.reportTable').get_text(strip=True)\n",
    "\n",
    "\n",
    "    data_dict = {\n",
    "        \"Sitting Date\": sitting_date,\n",
    "        \"Section Name\": section_name,\n",
    "        \"MPs Speaking\": mps_speaking,\n",
    "        \"Content\": text_content\n",
    "    }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "directory_path = f\"./{output_directory}\"\n",
    "\n",
    "data_json = f\"./{directory_path}/data.json\"\n",
    "\n",
    "# Create an empty list to store extracted content from all files\n",
    "all_data = []\n",
    "\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        with open(os.path.join(directory_path, file_name), 'r', encoding='utf-8') as txtfile:\n",
    "            content = txtfile.read()\n",
    "            data_dict = extract_content(content)\n",
    "            all_data.append(data_dict)\n",
    "\n",
    "# Save the list of extracted content to a JSON file\n",
    "with open(data_json, 'w', encoding='utf-8') as jsonfile:\n",
    "    json.dump(all_data, jsonfile, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Ang Wei Neng\n",
      "Mr Baey Yam Keng\n",
      "Miss Cheryl Chan Wei Ling\n",
      "Mr Chan Chun Sing\n",
      "Ms Usha Chandradas\n",
      "Mr Chee Hong Tat\n",
      "Mr Edward Chia Bing Hui\n",
      "Mr Chong Kee Hiong\n",
      "Mr Desmond Choo\n",
      "Mr Chua Kheng Wee Louis\n",
      "Mr Eric Chua\n",
      "Mr Keith Chua\n",
      "Mr Darryl David\n",
      "Mr Christopher de Souza\n",
      "Ms Foo Mee Har\n",
      "Ms Grace Fu Hai Yien\n",
      "Mr Gan Kim Yong\n",
      "Ms Gan Siow Huang\n",
      "Mr Gan Thiam Poh\n",
      "Mr Gerald Giam Yean Song\n",
      "Mr Derrick Goh\n",
      "Ms He Ting Ru\n",
      "Mr Heng Chee How\n",
      "Mr Heng Swee Keat\n",
      "Mr Shawn Huang Wei Zhong\n",
      "Ms Indranee Rajah\n",
      "Mr S Iswaran\n",
      "Dr Janil Puthucheary\n",
      "Dr Amy Khor Lean Suan\n",
      "Dr Koh Poh Koon\n",
      "Mr Kwek Hian Chuan Henry\n",
      "Mr Desmond Lee\n",
      "Mr Lee Hsien Loong\n",
      "Mr Mark Lee\n",
      "Mr Leong Mun Wai\n",
      "Mr Liang Eng Hwa\n",
      "Mr Lim Biow Chuan\n",
      "Assoc Prof Jamus Jerome Lim\n",
      "Ms Sylvia Lim\n",
      "Dr Lim Wee Kiak\n",
      "Ms Low Yen Ling\n",
      "Ms Mariam Jaafar\n",
      "Mr Masagos Zulkifli Bin Masagos Mohamad\n",
      "Dr Mohamad Maliki Bin Osman\n",
      "Mr Mohd Fahmi Bin Aliman\n",
      "Mr Muhamad Faisal Bin Abdul Manap\n",
      "Assoc Prof Dr Muhammad Faishal Ibrahim\n",
      "Mr Murali Pillai SC\n",
      "Ms Nadia Ahmad Samdin\n",
      "Dr Ng Eng Hen\n",
      "Ms Ng Ling Ling\n",
      "Mr Louis Ng Kok Kwang\n",
      "Mr Ong Hua Han\n",
      "Miss Rachel Ong\n",
      "Mr Ong Ye Kung\n",
      "Mr Neil Parekh Nimil Rajnikant\n",
      "Ms Joan Pereira\n",
      "Ms Denise Phua Lay Peng\n",
      "Ms Hazel Poa\n",
      "Ms Poh Li San\n",
      "Mr Pritam Singh\n",
      "Ms Rahayu Mahzam\n",
      "Assoc Prof Razwana Begum Abdul Rahim\n",
      "Mr Saktiandi Supaat\n",
      "Mr Seah Kian Peng\n",
      "Ms See Jinli Jean\n",
      "Mr K Shanmugam\n",
      "Mr Sharael Taha\n",
      "Ms Sim Ann\n",
      "Mr Sitoh Yih Pin\n",
      "Ms Hany Soh\n",
      "Ms Sun Xueling\n",
      "Dr Syed Harun Alhabsyi\n",
      "Mr Alvin Tan\n",
      "Ms Carrie Tan\n",
      "Mr Dennis Tan Lip Fong\n",
      "Mr Desmond Tan\n",
      "Ms Jessica Tan Soon Neo\n",
      "Mr Tan Kiat How\n",
      "Dr Tan See Leng\n",
      "Dr Tan Wu Meng\n",
      "Mr Patrick Tay Teck Guan\n",
      "Mr Teo Chee Hean\n",
      "Mrs Josephine Teo\n",
      "Mr Raj Joshua Thomas\n",
      "Ms Tin Pei Ling\n",
      "Mr Edwin Tong Chun Fai\n",
      "Mr Vikram Nair\n",
      "Dr Vivian Balakrishnan\n",
      "Dr Wan Rizal\n",
      "Mr Don Wee\n",
      "Mr Lawrence Wong\n",
      "Mr Xie Yao Quan\n",
      "Mr Alex Yam Ziming\n",
      "Ms Yeo Wan Ling\n",
      "Mr Yip Hon Weng\n",
      "Mr Melvin Yong Yik Chye\n",
      "Mr Zaqy Mohamad\n",
      "Mr Zhulkarnain Abdul Rahim\n"
     ]
    }
   ],
   "source": [
    "\n",
    "URL = \"https://www.parliament.gov.sg/mps/list-of-current-mps\"\n",
    "\n",
    "def scrape_mp():\n",
    "    response = requests.get(URL)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve webpage.\")\n",
    "        return []\n",
    "\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    \n",
    "    # define selectors\n",
    "    mp_data = {'Name':[], 'Constituency':[]}\n",
    "    mp_container = soup.find('ul', class_='list')\n",
    "\n",
    "    # find all 'li' elements inside the 'ul' container\n",
    "    mp_items = mp_container.find_all('li') if mp_container else []\n",
    "    \n",
    "    for container in mp_items:\n",
    "        name_element = container.find('a')\n",
    "        constituency_element = container.find('div', class_=\"col-md-6 col-xs-11 mp-sort constituency\")\n",
    "        print(name_element.text.strip())\n",
    "\n",
    "        if name_element and constituency_element:\n",
    "            name = name_element.text.strip()\n",
    "            constituency = constituency_element.text.strip()\n",
    "            mp_data['Name'].append(name)\n",
    "            \n",
    "            mp_data['Constituency'].append(constituency)\n",
    "    return mp_data\n",
    "\n",
    "mp_data = scrape_mp()\n",
    "# Define the path for the JSON file\n",
    "filename = './scraped_data/mp.json'\n",
    "\n",
    "# Write the dictionary to the file\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(mp_data, file)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
